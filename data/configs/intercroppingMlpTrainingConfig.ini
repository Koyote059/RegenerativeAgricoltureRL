[training]
# Number of training epochs
epochs = 100
# Size of training batches
batch_size = 32
# Initial learning rate for the optimizer
learning_rate = 0.001
# Weight decay for regularization
weight_decay = 0.01
seed = 21563

[checkpoints]
# Save model checkpoint every N epochs
save_frequency = 5
# Directory path to save model checkpoints
save_path = ./checkpoints/intercropping_mlp
# Directory path to save training logs
log_path = ./logs/intercropping_mlp

[early_stopping]
# Number of epochs to wait before early stopping
early_stop_patience = 15
# Minimum improvement required to reset early stopping patience
early_stop_min_improvement = 0.001

[data_augmentation]
# Rate at which to sample crop swapping augmentation
crop_swap_sample_rate = 0.2
# Rate at which to sample masking augmentation
masking_sample_rate = 0.15
# Probability of masking individual features when applying masking
masking_probability = 0.25

[dataset]
# Train, Val, Test ratio
data_split = (0.7, 0.2, 0.1)

[model]
# Nr of neurons of each hidden layres. The list can be any number >= 1
hidden_layers_sizes = (128, 64)
# Activation function to use between hidden layers. Options: 'leakyrelu', 'relu', 'sigmoid', 'tanh', 'softmax'
activation_function = LeakyReLu

[seed]
# Random seed for reproducibility
random_seed = 42